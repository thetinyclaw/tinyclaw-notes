# 2026-02-11

- Set up local LLM runtime on the M4 Mac mini (16GB) using **Ollama via Homebrew**.
  - OS: **macOS 15.3.1 (24D70)**, arch **arm64**.
  - Installed: `brew install ollama` and started service.
- Pulled models:
  - `gemma3:12b` (Q4_K_M)
  - `qwen2.5-coder:14b` (Q4_K_M)
  - `deepseek-r1:8b` (Q4_K_M) â€” user requested Q6, but Ollama library tags for 8B appear to offer q4/q8/fp16 (no q6).
- Smoke tests:
  - Gemma: responded `SMOKE_OK gemma`
  - Qwen: responded `SMOKE_OK qwen`
  - DeepSeek: verified via API `/api/generate` returned `SMOKE_OK deepseek` (CLI runs were finicky due to thinking output / interactivity).
- Noted recurring warning in Ollama output: `MLX: Failed to load symbol: mlx_metal_device_info`.
  - Not blocking inference; Ollama runner reports Metal device (Apple M4) and starts normally.
